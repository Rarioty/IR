#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass IEEEtran-CompSoc
\begin_preamble
\usepackage[style=authoryear,backend=biber]{biblatex}
\addbibresource{references.bib}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command biber
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
LU, QR and Cholesky factorizations: Programming Model, Performance Analysis
 and Optimization Techniques for the Intel Knights Landing Xeon Phi
\end_layout

\begin_layout Author
DUFOYER Benjamin, CHEVALIER Arthur
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Today's modern computers have a wide variety of heterogeneous compute resources,
 ranging from multicore CPUs to GPUs and to face up to the increase quantity
 of numerical informations and modern systems complexity, it is necessary
 to enhance the structures that manage this task.
 The main thing of the heterogeous system is the capacity of every ressources
 to work in the same time.
 A good task scheduling is paramount to increase performance on this type
 of system.
 The biggest problem is to chop every task to execute in the same time while
 avoiding the competitor accesses.
 With the multiplication of core number, the developers must change their
 way of programming in order to slice the problem to be able to parrallelize
 their application with high-performance.
 Intel released a new Xeon PHI called KNL (Knight Landing) which has up
 to 72 cores and this paper is about testing a library called MAGMA in this
 new highly parallelizable platform.
 In the matrix factorizations algorithm the MAGMA library provide a way
 to slice the matrix to ensure that all the tasks will run at the same time
 and with minimal idle time.
 The library also use a way to divide all the cores of the KNL in order
 to simulate multiple devices to use.
 The article will compare performances over three different matrix factorization
 algorithms: LU, QR and Cholesky.
\end_layout

\begin_layout Standard
Presentation of KNL, KNL is the new version of Xeon Phi.
 He is on the Intel Family of Many Integrated Core architecture (MIC).
 He is available in two form, coprocessor and host processor.
 He will be built using up to 72 cores with four threads per core and use
 LGA 3647 socket.
 Each core are engrave at 14nm.
 It supports for up 384Gb of DDR4 RAM and for up 16Gb of MCDRAM.
 The MCDRAM is a innovation for the Xeon, it's a 3D-stacked DRAM.
 The favours of the RAM is the fast access to data (better than classical
 RAM) on the test, they use the MCDRAM in flat mode, so the MCDRAM have
 her own adress space and can be adress explicitly.
 The next innovation of the KNL is the addition of two 512-bit vectorial
 unit on each core.
 The KNL achieave a double precision theorical peak of 2662 Gflops/s.
 The version used in the test is the selfhosted preproduction Intel Xeon
 Phi card with 16Gb of MCDRAM, each core run at 1,3 GHz.
 It consists of 64 cores with 4 hyperthreads each.
 The KNL have different mod of utilisation.
 You can use in InterConnect mode, in this mode, all core are connected
 with his neighbor's.
 Other mode are Cluster Mode with a "All to All", quadrant and Sub-Numa.
 In the test, they use the Quadrant Mode to increase performance.
 The principe is simple, the card is cut into four part, every part have
 a unique Master and the core remaining are a slave.
\end_layout

\begin_layout Section
Contribution
\end_layout

\begin_layout Standard
The MAGMA (Matrix Algebra on GPU and Multi-core Architectures) project aims
 to develop a dense linear algebra library similar to LAPACK but for heterogeneo
us/hybrid architectures, starting with current “Multi-core and GPU” systems.
 The goal of MAGMA is to combine the strengths of different algorithms within
 a single framework.
 They want to design linear algebra algorithms and frameworks for hybrid
 manycore and GPUs systemes that can enable applications to fully exploit
 the power that each of the hybrid components offer.
 In practice MAGMA want to reduce the communication into every function
 to have a better parallelism.
\end_layout

\begin_layout Section
Implementation elements
\end_layout

\begin_layout Standard
MAGMA actually uses the BLAS libraries to compute the LU, QR and Cholesky
 matrix factorizations.
 The algorithm works as follow:
\end_layout

\begin_layout Standard
First we factorize the first row and column of the matrix and when we finish
 this task we update the trailing matrix on the GPU.
 To use parallelism on this algorithm the idea is to start to factorize
 the first line and column of the trailing matrix as soon as possible so
 when the GPU updated them.
 The factorization works on the CPU to maximize the performances.
 The BLAS2 library is used on the factorization of single vectors (column
 and row) and the BLAS3 is used on the update of the trailing matrix.
 We can also vary the depth level of the algorithm because we can have at
 the same time multiple updating task on the trailing matrix.
 When the second factorization ended we launch a second update on the trailing
 matrix while the first is still running.
 The number of updating tasks is the depth level.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename algorithm_figure.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Two-phase implementation of a one-sided factorization
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Validation/Experiments
\end_layout

\begin_layout Standard
Three different configurations has been used for the performances tests:
\end_layout

\begin_layout Itemize
The first one configuration has two 18-core Intel Xeon E5-2697 (Broadwell)
 processors, running at 2,6 GHz.
 Each socket as 35 MiB of shared L3 cache, and each core has a private 3,5
 MiB L2 and 448 KB L1 cache.
 The system is equipped with 52 GB of memory and the theoretical peak in
 double precision is 20,8 Gflop/s per core.
\end_layout

\begin_layout Itemize
The second system is equipped with two 8-core Intel Xeon E5-2670 (Sandy
 Bridge) processors and two Intel Xeon Phi 7120 (KNC) cards with 15,8 GB
 per card.
 She is equip with 61 core runng at 1,23GHz.
 And achieving a double precision theoretical peak of 1180 Gflops/s.
 The KNC is the old version of Xeon Phi and don't have MCDRAM and vector
 unit.
\end_layout

\begin_layout Itemize
The last is a selfhosted preproduction Intel Xeon Phi Knights Landing (KNL)
 card, with 16 GB of MCDRAM used in flat mode, running at 1.3GHz and achieving
 a double precision at a theorical peak of 2662 Gflop/s.
 It has 64 cores with 4 hyper-threads each.
\end_layout

\begin_layout Standard
The tests are performed over the three differents factorizations algorithms
 with varying matrix size (from 2k to 40k cells for border size).
 For the LU factorization we can see that behind the 6k size the two KNL
 and the first one are far over the others with aproximatively twice the
 performance.
 The KNL reached its limit of performance at 28k with a peak of 1500Gflop/s
 et come behind the second configuration that reach the 1700Gflop/s with
 the 40k matrix.
 We can also see that the KNL cannot perform tests over 30k matrix because
 of its memory.
\end_layout

\begin_layout Standard
For the QR factorization we have the same schema, the third configuration
 get to follow the KNL until 8k matrix size but get overtaked with taller
 matrix.
 With medium matrix size the KNL is still leading with the third configuration
 like the LU factorization.
\end_layout

\begin_layout Standard
Finally for the Cholesky factorization the second config is ahead on small
 matrix (6k size) but the KNL leads the way until 16k matrix size and the
 last configuration (two KNC and one processor) get better performance on
 large matrix.
\end_layout

\begin_layout Standard
To sum up this differents tests, the new Xeon Phi proposed by Intel obtain
 better performances than the older version (KNC) but is not uneatable on
 performance contest.
 We can see that it obtains remarquable performances on small matrix but
 we easily reached the limit of performance on medium matrix.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename LU_performances.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
LU performances with MAGMA
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename QR_performances.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
QR performances with MAGMA
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Cholesky_performances.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Cholesky performances with MAGMA
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Subsection
Discussion
\end_layout

\begin_layout Standard
This article explains the differents optimisations technics on the matrix
 factorization LU, QR and Cholesky on many-cores systems.
 The authors show how wise modifications of task management can bring higher
 performances on heterogeneous systems while using the same alogirthm pattern
 on all platforms.
 The performances here allow us to realize that their approach is very good
 on many-cores systems and make it possible to get high-performance with
 high scalability along with easy development on different algorithms.
 Concerning the KNL, we can see that this platform is better than the others
 on some points compared to the present configurations but show its limits
 in the actual version.
 We must remind that this is just a preproduction version and maybe for
 the future ones they will fix the impossibility to make tests on bigger
 matrix than 30k of size.
\end_layout

\begin_layout Subsection
Short term perspectives
\end_layout

\begin_layout Standard
At this time, the KNL is in testing phase.
 This paper is about testing matrices factorizations on it and a lot of
 others performances tests will appear soon.
\end_layout

\begin_layout Subsection
Long term perspectives
\end_layout

\begin_layout Standard
In the long term, the KNL will be used in clusters for high performance
 processing 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "references"
options "plain"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printbibliography
\end_layout

\end_inset


\end_layout

\end_body
\end_document
